#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ù…Ø­Ø³Ù† Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
Enhanced Shamela Scraper V2 - Advanced Async & Multiprocessing

Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†Ø© Ù…Ø¹ aiohttp
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª  
- ØªØ­Ø³ÙŠÙ† HTTP session Ù…ØªÙ‚Ø¯Ù…
- Ù…Ø¹Ø§Ù„Ø¬Ø© HTML Ø³Ø±ÙŠØ¹Ø© Ù…Ø¹ lxml
- Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„Ù„ØªØ¨Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„ÙƒØªØ§Ø¨
- Ø¥Ø¯Ø§Ø±Ø© Ø°Ø§ÙƒØ±Ø© Ù…Ø­Ø³Ù†Ø©
"""

import asyncio
import aiohttp
import time
import json
import sys
import argparse
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
from typing import List, Dict, Any, Optional, Tuple
import psutil
import os
from dataclasses import dataclass, asdict
from pathlib import Path

try:
    from lxml import html as lxml_html, etree
    LXML_AVAILABLE = True
except ImportError:
    LXML_AVAILABLE = False
    print("âš ï¸  lxml ØºÙŠØ± Ù…ØªÙˆÙØ± - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup (Ø£Ø¨Ø·Ø£)")

from bs4 import BeautifulSoup
import mysql.connector
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
@dataclass
class AdvancedPerformanceConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª HTTP Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    max_connections: int = 100
    max_connections_per_host: int = 30
    connection_timeout: float = 15.0
    read_timeout: float = 30.0
    total_timeout: float = 60.0
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
    async_semaphore_limit: int = 15
    async_batch_size: int = 50
    retry_attempts: int = 5
    retry_delay: float = 0.5
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
    multiprocessing_threshold: int = 200
    max_processes: int = None
    process_chunk_size: int = 100
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
    enable_http2: bool = True
    enable_compression: bool = True
    enable_keepalive: bool = True
    dns_cache_ttl: int = 300
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    memory_efficient: bool = False
    max_memory_mb: int = 2048
    gc_threshold: int = 1000
    
    def __post_init__(self):
        if self.max_processes is None:
            self.max_processes = min(mp.cpu_count(), 12)

class AdvancedHTTPSession:
    """Ø¬Ù„Ø³Ø© HTTP Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Ø¯Ø¹Ù… HTTP/2 ÙˆÙ…Ø¶Ø§Ø¹ÙØ© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª"""
    
    def __init__(self, config: AdvancedPerformanceConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.connector: Optional[aiohttp.TCPConnector] = None
        
    async def __aenter__(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¯Ø®ÙˆÙ„"""
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØµÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        self.connector = aiohttp.TCPConnector(
            limit=self.config.max_connections,
            limit_per_host=self.config.max_connections_per_host,
            ttl_dns_cache=self.config.dns_cache_ttl,
            use_dns_cache=True,
            keepalive_timeout=60 if self.config.enable_keepalive else 0,
            enable_cleanup_closed=True,
            force_close=not self.config.enable_keepalive,
            ssl=False  # Ù„Ù€ HTTP ÙÙ‚Ø· ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ù‡Ù„Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        timeout = aiohttp.ClientTimeout(
            total=self.config.total_timeout,
            connect=self.config.connection_timeout,
            sock_read=self.config.read_timeout
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate' if self.config.enable_compression else 'identity',
            'Connection': 'keep-alive' if self.config.enable_keepalive else 'close',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers=headers
        )
        
        return self.session
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¬Ù„Ø³Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø±ÙˆØ¬"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()

class FastHTMLProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ HTML Ø³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… lxml Ø£Ùˆ BeautifulSoup"""
    
    @staticmethod
    def extract_page_content(html: str, page_num: int) -> Optional[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù…ÙƒÙ†Ø©"""
        try:
            if LXML_AVAILABLE:
                return FastHTMLProcessor._extract_with_lxml(html, page_num)
            else:
                return FastHTMLProcessor._extract_with_bs4(html, page_num)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© HTML Ù„Ù„ØµÙØ­Ø© {page_num}: {e}")
            return None
    
    @staticmethod
    def _extract_with_lxml(html: str, page_num: int) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… lxml (Ø§Ù„Ø£Ø³Ø±Ø¹)"""
        try:
            tree = lxml_html.fromstring(html)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            content_elements = tree.xpath("//div[@class='nass']")
            if not content_elements:
                content_elements = tree.xpath("//div[contains(@class, 'text')]")
            
            if content_elements:
                content_elem = content_elements[0]
                text_content = content_elem.text_content().strip()
                html_content = lxml_html.tostring(content_elem, encoding='unicode', method='html')
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
                word_count = len(text_content.split())
                char_count = len(text_content)
                
                return {
                    'page_number': page_num,
                    'content': text_content,
                    'html_content': html_content,
                    'word_count': word_count,
                    'char_count': char_count,
                    'extracted_at': datetime.now().isoformat(),
                    'extraction_method': 'lxml'
                }
            
        except Exception:
            # ØªØ±Ø§Ø¬Ø¹ Ø¥Ù„Ù‰ BeautifulSoup
            pass
        
        return FastHTMLProcessor._extract_with_bs4(html, page_num)
    
    @staticmethod
    def _extract_with_bs4(html: str, page_num: int) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup (Ø¨Ø¯ÙŠÙ„)"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_div = soup.find('div', class_='nass')
        if not content_div:
            content_div = soup.find('div', class_=re.compile(r'text|content'))
        
        if content_div:
            text_content = content_div.get_text().strip()
            html_content = str(content_div)
            
            return {
                'page_number': page_num,
                'content': text_content,
                'html_content': html_content,
                'word_count': len(text_content.split()),
                'char_count': len(text_content),
                'extracted_at': datetime.now().isoformat(),
                'extraction_method': 'beautifulsoup'
            }
        
        return {
            'page_number': page_num,
            'content': '',
            'html_content': '',
            'word_count': 0,
            'char_count': 0,
            'extracted_at': datetime.now().isoformat(),
            'extraction_method': 'failed'
        }

class AsyncPageExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„ØµÙØ­Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†"""
    
    def __init__(self, config: AdvancedPerformanceConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    async def extract_pages_batch(self, book_id: int, page_range: Tuple[int, int], 
                                 session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
        start_page, end_page = page_range
        semaphore = asyncio.Semaphore(self.config.async_semaphore_limit)
        
        async def extract_single_page(page_num: int) -> Optional[Dict[str, Any]]:
            async with semaphore:
                for attempt in range(self.config.retry_attempts):
                    try:
                        url = f"https://shamela.ws/book/{book_id}/{page_num}"
                        
                        async with session.get(url) as response:
                            if response.status == 200:
                                html = await response.text()
                                result = FastHTMLProcessor.extract_page_content(html, page_num)
                                if result:
                                    self.logger.debug(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ø¬Ø­ Ù„Ù„ØµÙØ­Ø© {page_num}")
                                    return result
                            
                            elif response.status == 404:
                                self.logger.warning(f"âŒ ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {page_num}")
                                return None
                            
                        await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                        
                    except Exception as e:
                        if attempt == self.config.retry_attempts - 1:
                            self.logger.error(f"âŒ ÙØ´Ù„ Ù†Ù‡Ø§Ø¦ÙŠ ÙÙŠ ØµÙØ­Ø© {page_num}: {e}")
                        else:
                            await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                
                return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
        tasks = [extract_single_page(page) for page in range(start_page, end_page + 1)]
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ù…Ø¹ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØµØ­ÙŠØ­Ø©
        valid_results = []
        for result in results:
            if isinstance(result, dict) and result is not None:
                valid_results.append(result)
            elif isinstance(result, Exception):
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {result}")
        
        return valid_results

class MultiprocessExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ù„Ù„ÙƒØªØ¨ Ø§Ù„Ø¶Ø®Ù…Ø©"""
    
    def __init__(self, config: AdvancedPerformanceConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def extract_book_parallel(self, book_id: int, total_pages: int) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª"""
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙØ­Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø·Ø¹
        chunks = self._create_page_chunks(total_pages)
        
        self.logger.info(f"ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©: {len(chunks)} Ø¯ÙØ¹Ø© Ø¹Ù„Ù‰ {self.config.max_processes} Ø¹Ù…Ù„ÙŠØ©")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
        all_pages = []
        with ProcessPoolExecutor(max_workers=self.config.max_processes) as executor:
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‡Ø§Ù…
            future_to_chunk = {
                executor.submit(extract_chunk_worker, book_id, chunk, asdict(self.config)): chunk 
                for chunk in chunks
            }
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            for i, future in enumerate(as_completed(future_to_chunk)):
                chunk = future_to_chunk[future]
                try:
                    chunk_pages = future.result(timeout=600)  # 10 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„ÙƒÙ„ Ø¯ÙØ¹Ø©
                    all_pages.extend(chunk_pages)
                    
                    start_page, end_page = chunk
                    self.logger.info(f"âœ… Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¯ÙØ¹Ø© {i+1}/{len(chunks)} "
                                   f"(ØµÙØ­Ø§Øª {start_page}-{end_page}): {len(chunk_pages)} ØµÙØ­Ø©")
                    
                except Exception as e:
                    start_page, end_page = chunk
                    self.logger.error(f"âŒ ÙØ´Ù„Øª Ø§Ù„Ø¯ÙØ¹Ø© {i+1} (ØµÙØ­Ø§Øª {start_page}-{end_page}): {e}")
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        return sorted(all_pages, key=lambda x: x.get('page_number', 0))
    
    def _create_page_chunks(self, total_pages: int) -> List[Tuple[int, int]]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙØ­Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©"""
        chunk_size = max(self.config.process_chunk_size, 
                        total_pages // self.config.max_processes)
        
        chunks = []
        for i in range(1, total_pages + 1, chunk_size):
            end = min(i + chunk_size - 1, total_pages)
            chunks.append((i, end))
        
        return chunks

def extract_chunk_worker(book_id: int, page_range: Tuple[int, int], 
                        config_dict: dict) -> List[Dict[str, Any]]:
    """Ø¹Ø§Ù…Ù„ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø·Ø¹Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ù…Ù†ÙØµÙ„Ø©"""
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø¥Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = AdvancedPerformanceConfig(**config_dict)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¹Ù…Ù„ÙŠØ©
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        return loop.run_until_complete(extract_chunk_async(book_id, page_range, config))
    finally:
        loop.close()

async def extract_chunk_async(book_id: int, page_range: Tuple[int, int], 
                             config: AdvancedPerformanceConfig) -> List[Dict[str, Any]]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø·Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… async ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ù…Ù†ÙØµÙ„Ø©"""
    extractor = AsyncPageExtractor(config)
    
    async with AdvancedHTTPSession(config) as session:
        return await extractor.extract_pages_batch(book_id, page_range, session)

class BookInfoExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    
    @staticmethod
    async def get_book_info(book_id: int, session: aiohttp.ClientSession) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        try:
            url = f"https://shamela.ws/book/{book_id}"
            
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    return BookInfoExtractor._parse_book_info(html, book_id)
                    
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ {book_id}: {e}")
        
        return BookInfoExtractor._get_default_book_info(book_id)
    
    @staticmethod  
    def _parse_book_info(html: str, book_id: int) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ù…Ù† HTML"""
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ù„ØµÙØ­Ø§Øª
        # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
        
        if LXML_AVAILABLE:
            try:
                tree = lxml_html.fromstring(html)
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙØ­Ø§Øª Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø®Ø±Ù‰
                page_links = tree.xpath("//a[contains(@href, '/book/%d/')]" % book_id)
                estimated_pages = len(page_links) if page_links else 100
            except:
                estimated_pages = 100
        else:
            soup = BeautifulSoup(html, 'html.parser')
            page_links = soup.find_all('a', href=re.compile(f'/book/{book_id}/'))
            estimated_pages = len(page_links) if page_links else 100
        
        return {
            'book_id': book_id,
            'estimated_pages': max(estimated_pages, 50),  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ 50 ØµÙØ­Ø©
            'title': f'ÙƒØªØ§Ø¨ Ø±Ù‚Ù… {book_id}',  # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡
            'author': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',  # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡
            'extraction_date': datetime.now().isoformat()
        }
    
    @staticmethod
    def _get_default_book_info(book_id: int) -> Dict[str, Any]:
        """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„ÙƒØªØ§Ø¨"""
        return {
            'book_id': book_id,
            'estimated_pages': 100,
            'title': f'ÙƒØªØ§Ø¨ Ø±Ù‚Ù… {book_id}',
            'author': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
            'extraction_date': datetime.now().isoformat()
        }

class AdvancedShamelaScraper:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©"""
    
    def __init__(self, config: AdvancedPerformanceConfig = None):
        self.config = config or AdvancedPerformanceConfig()
        self.logger = self._setup_logging()
        
        # Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        self.async_extractor = AsyncPageExtractor(self.config)
        self.multiprocess_extractor = MultiprocessExtractor(self.config)
    
    def _setup_logging(self) -> logging.Logger:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„Ù
            file_handler = logging.FileHandler('enhanced_shamela_v2.log', encoding='utf-8')
            file_formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - [%(processName)s-%(threadName)s] - %(message)s'
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
            
            # Ù…Ø¹Ø§Ù„Ø¬ ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ…
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        return logger
    
    async def extract_book(self, book_id: int, **kwargs) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„ Ø¨Ø£Ù‚ØµÙ‰ Ø³Ø±Ø¹Ø© Ù…Ù…ÙƒÙ†Ø©"""
        start_time = time.time()
        
        self.logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒØªØ§Ø¨ {book_id}")
        
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨
            async with AdvancedHTTPSession(self.config) as session:
                book_info = await BookInfoExtractor.get_book_info(book_id, session)
                total_pages = book_info['estimated_pages']
                
                self.logger.info(f"ğŸ“š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨: {total_pages} ØµÙØ­Ø© Ù…ØªÙˆÙ‚Ø¹Ø©")
                
                # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…
                if total_pages <= self.config.multiprocessing_threshold:
                    self.logger.info(f"ğŸ“– ÙƒØªØ§Ø¨ ØµØºÙŠØ±/Ù…ØªÙˆØ³Ø· - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©")
                    pages = await self._extract_async_method(book_id, total_pages, session)
                else:
                    self.logger.info(f"ğŸ“š ÙƒØªØ§Ø¨ Ø¶Ø®Ù… - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª")
                    pages = self.multiprocess_extractor.extract_book_parallel(book_id, total_pages)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            elapsed = time.time() - start_time
            pages_per_second = len(pages) / elapsed if elapsed > 0 else 0
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            total_words = sum(page.get('word_count', 0) for page in pages)
            total_chars = sum(page.get('char_count', 0) for page in pages)
            
            result = {
                'book_id': book_id,
                'book_info': book_info,
                'pages': pages,
                'statistics': {
                    'total_pages': len(pages),
                    'extraction_time': elapsed,
                    'pages_per_second': pages_per_second,
                    'total_words': total_words,
                    'total_chars': total_chars,
                    'average_words_per_page': total_words / len(pages) if pages else 0,
                    'extraction_method': 'multiprocessing' if total_pages > self.config.multiprocessing_threshold else 'async'
                },
                'config_used': asdict(self.config),
                'extraction_date': datetime.now().isoformat()
            }
            
            self.logger.info(f"ğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {len(pages)} ØµÙØ­Ø© ÙÙŠ {elapsed:.2f} Ø«Ø§Ù†ÙŠØ©")
            self.logger.info(f"âš¡ Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©: {pages_per_second:.2f} ØµÙØ­Ø©/Ø«Ø§Ù†ÙŠØ©")
            self.logger.info(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {total_words:,} ÙƒÙ„Ù…Ø©")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ {book_id}: {e}")
            raise
    
    async def _extract_async_method(self, book_id: int, total_pages: int, 
                                   session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©"""
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
        batch_size = self.config.async_batch_size
        all_pages = []
        
        for i in range(1, total_pages + 1, batch_size):
            end_page = min(i + batch_size - 1, total_pages)
            
            self.logger.info(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø©: ØµÙØ­Ø§Øª {i}-{end_page}")
            
            batch_pages = await self.async_extractor.extract_pages_batch(
                book_id, (i, end_page), session
            )
            
            all_pages.extend(batch_pages)
            
            self.logger.info(f"âœ… Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¯ÙØ¹Ø©: {len(batch_pages)} ØµÙØ­Ø©")
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if self.config.memory_efficient and len(all_pages) % self.config.gc_threshold == 0:
                import gc
                gc.collect()
        
        return sorted(all_pages, key=lambda x: x.get('page_number', 0))

async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ
    config = AdvancedPerformanceConfig(
        max_connections=150,
        max_connections_per_host=50,
        async_semaphore_limit=20,
        multiprocessing_threshold=100,
        max_processes=8
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
    scraper = AdvancedShamelaScraper(config)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ ÙƒØªØ§Ø¨
    book_id = int(sys.argv[1]) if len(sys.argv) > 1 else 41
    
    try:
        result = await scraper.extract_book(book_id)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        output_file = f"book_{book_id}_advanced_v2.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_file}")
        print(f"ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        stats = result['statistics']
        print(f"   - Ø§Ù„ØµÙØ­Ø§Øª: {stats['total_pages']}")
        print(f"   - Ø§Ù„ÙˆÙ‚Øª: {stats['extraction_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"   - Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['pages_per_second']:.2f} ØµÙØ­Ø©/Ø«Ø§Ù†ÙŠØ©")
        print(f"   - Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {stats['total_words']:,}")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
