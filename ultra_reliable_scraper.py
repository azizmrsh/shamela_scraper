#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© - Ultra Reliable Scraper
Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100% Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡
"""

import time
import logging
import traceback
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
import json
import gzip
import os
from datetime import datetime

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
try:
    from enhanced_shamela_scraper import scrape_enhanced_book, PerformanceConfig, Book
    from enhanced_database_manager import save_enhanced_json_to_database
except ImportError as e:
    print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯: {e}")
    print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")

@dataclass
class ReliabilityConfig:
    """ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„ÙØ§Ø¦Ù‚Ø©"""
    max_retries: int = 5  # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    retry_delay: float = 2.0  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
    exponential_backoff: bool = True  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ£Ø®ÙŠØ± ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹
    verify_extraction: bool = True  # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    auto_recovery: bool = True  # Ø§Ø³ØªØ±Ø¯Ø§Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠ
    save_progress: bool = True  # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø¯Ù…
    detailed_logging: bool = True  # Ø³Ø¬Ù„Ø§Øª Ù…ÙØµÙ„Ø©

class UltraReliableScraper:
    """Ø³ÙƒØ±Ø¨Øª ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
    
    def __init__(self, config: ReliabilityConfig = None):
        self.reliability_config = config or ReliabilityConfig()
        self.setup_logging()
        self.failed_pages = []
        self.progress_file = None
        
    def setup_logging(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ù…ÙØµÙ„"""
        log_format = '%(asctime)s - [%(levelname)s] - %(funcName)s:%(lineno)d - %(message)s'
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        os.makedirs('logs', exist_ok=True)
        
        # ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        logging.basicConfig(
            level=logging.DEBUG if self.reliability_config.detailed_logging else logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(f'logs/ultra_reliable_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸ›¡ï¸ ØªÙ… ØªÙØ¹ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„ÙØ§Ø¦Ù‚Ø©")

    def get_optimal_config(self, estimated_pages: int = None) -> PerformanceConfig:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ù…Ø«Ù„ ÙˆØ§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ø§Ù‹"""
        config = PerformanceConfig()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© ÙˆÙ…Ø¬Ø±Ø¨Ø©
        config.use_async = False  # Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ù…Ø¶Ù…ÙˆÙ†Ø©
        config.use_lxml = True
        config.enable_caching = True
        config.max_retries = 3
        config.timeout = 30  # ÙˆÙ‚Øª Ø§Ù†ØªØ¸Ø§Ø± ÙƒØ§ÙÙŠ
        config.request_delay = 0.5  # ØªØ£Ø®ÙŠØ± Ø¢Ù…Ù† Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        
        # ØªÙƒÙˆÙŠÙ† Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù‚Ø¯Ø±
        if estimated_pages and estimated_pages > 1000:
            # ÙƒØªØ¨ Ø¶Ø®Ù…Ø© - Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø§ÙØ¸Ø©
            config.max_workers = 8
            config.batch_size = 1000
            config.connection_pool_size = 8
            config.request_delay = 1.0
        elif estimated_pages and estimated_pages > 500:
            # ÙƒØªØ¨ ÙƒØ¨ÙŠØ±Ø© - Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø©
            config.max_workers = 12
            config.batch_size = 1500
            config.connection_pool_size = 12
            config.request_delay = 0.7
        else:
            # ÙƒØªØ¨ ØµØºÙŠØ±Ø©-Ù…ØªÙˆØ³Ø·Ø© - Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø³Ø±ÙŠØ¹Ø©
            config.max_workers = 16
            config.batch_size = 2000
            config.connection_pool_size = 16
            config.request_delay = 0.5
            
        self.logger.info(f"âš™ï¸ ØªÙƒÙˆÙŠÙ† Ù…ÙØ­Ø³ÙÙ‘Ù†: workers={config.max_workers}, delay={config.request_delay}s")
        return config

    def verify_book_extraction(self, book: Book, expected_pages: int = None) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        try:
            # ÙØ­ÙˆØµØ§Øª Ø£Ø³Ø§Ø³ÙŠØ©
            if not book:
                self.logger.error("âŒ Ø§Ù„ÙƒØªØ§Ø¨ ÙØ§Ø±Øº")
                return False
                
            if not book.title:
                self.logger.error("âŒ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØªØ§Ø¨ ÙØ§Ø±Øº")
                return False
                
            if not book.pages:
                self.logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙØ­Ø§Øª Ù…Ø³ØªØ®Ø±Ø¬Ø©")
                return False
                
            # ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            empty_pages = [p for p in book.pages if not p.content or len(p.content.strip()) < 10]
            if len(empty_pages) > len(book.pages) * 0.5:  # Ø£ÙƒØ«Ø± Ù…Ù† 50% ÙØ§Ø±ØºØ©
                self.logger.error(f"âŒ {len(empty_pages)} ØµÙØ­Ø© ÙØ§Ø±ØºØ© Ù…Ù† {len(book.pages)}")
                return False
                
            # ÙØ­Øµ Ø§Ù„ØªØ³Ù„Ø³Ù„
            page_numbers = [p.page_number for p in book.pages]
            if len(page_numbers) != len(set(page_numbers)):
                self.logger.error("âŒ Ø£Ø±Ù‚Ø§Ù… ØµÙØ­Ø§Øª Ù…ÙƒØ±Ø±Ø©")
                return False
                
            # ÙØ­Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            arabic_pages = 0
            for page in book.pages[:min(5, len(book.pages))]:  # ÙØ­Øµ Ø£ÙˆÙ„ 5 ØµÙØ­Ø§Øª
                if any('\u0600' <= char <= '\u06FF' for char in page.content):
                    arabic_pages += 1
                    
            if arabic_pages == 0:
                self.logger.warning("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ ÙÙŠ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙØ­ÙˆØµØ©")
                
            self.logger.info(f"âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù†Ø¬Ø­: {len(book.pages)} ØµÙØ­Ø©ØŒ {len(empty_pages)} ÙØ§Ø±ØºØ©")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚: {str(e)}")
            return False

    def extract_with_retry(self, book_id: str, max_pages: int = None, config: PerformanceConfig = None) -> Optional[Book]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        
        for attempt in range(self.reliability_config.max_retries + 1):
            try:
                self.logger.info(f"ğŸ”„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{self.reliability_config.max_retries + 1} Ù„Ù„ÙƒØªØ§Ø¨ {book_id}")
                
                # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
                book = scrape_enhanced_book(book_id, max_pages=max_pages, extract_content=True, config=config)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©
                if self.reliability_config.verify_extraction:
                    if not self.verify_book_extraction(book, max_pages):
                        raise Exception("ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬")
                
                self.logger.info(f"âœ… Ù†Ø¬Ø­ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ {book_id} ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}")
                return book
                
            except Exception as e:
                self.logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {str(e)}")
                
                if attempt < self.reliability_config.max_retries:
                    # Ø­Ø³Ø§Ø¨ Ø²Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ±
                    if self.reliability_config.exponential_backoff:
                        delay = self.reliability_config.retry_delay * (2 ** attempt)
                    else:
                        delay = self.reliability_config.retry_delay
                    
                    self.logger.info(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {delay} Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©...")
                    time.sleep(delay)
                    
                    # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù„Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
                    if config:
                        config.request_delay += 0.5  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ£Ø®ÙŠØ±
                        config.max_workers = max(4, config.max_workers - 2)  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…Ø§Ù„
                        self.logger.info(f"ğŸ”§ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ†: delay={config.request_delay}, workers={config.max_workers}")
                else:
                    self.logger.error(f"ğŸ’¥ ÙØ´Ù„ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ {book_id} Ø¨Ø¹Ø¯ {self.reliability_config.max_retries + 1} Ù…Ø­Ø§ÙˆÙ„Ø©")
                    
        return None

    def save_with_verification(self, book: Book, output_dir: str) -> Optional[str]:
        """Ø­ÙØ¸ Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ù„Ù"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ultra_reliable_book_{book.shamela_id}_{timestamp}.json"
            filepath = os.path.join(output_dir, filename)
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ dict Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© - Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
            def safe_convert(obj, attr, default=None):
                """ØªØ­ÙˆÙŠÙ„ Ø¢Ù…Ù† Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©"""
                if not hasattr(obj, attr):
                    return default
                value = getattr(obj, attr)
                if value is None:
                    return default
                if hasattr(value, 'name'):
                    return value.name
                if isinstance(value, (str, int, bool, float)):
                    return value
                if isinstance(value, list):
                    return [item.name if hasattr(item, 'name') else str(item) for item in value]
                return str(value)
            
            book_data = {
                "title": book.title,
                "shamela_id": book.shamela_id,
                "authors": [{"name": author.name} for author in book.authors] if book.authors else [],
                "publisher": safe_convert(book, 'publisher'),
                "book_section": safe_convert(book, 'book_section'),
                "edition": safe_convert(book, 'edition'),
                "edition_number": safe_convert(book, 'edition_number'),
                "publication_year": safe_convert(book, 'publication_year'),
                "edition_date_hijri": safe_convert(book, 'edition_date_hijri'),
                "page_count": safe_convert(book, 'page_count'),
                "volume_count": safe_convert(book, 'volume_count'),
                "categories": safe_convert(book, 'categories', []),
                "description": safe_convert(book, 'description'),
                "source_url": safe_convert(book, 'source_url'),
                "has_original_pagination": safe_convert(book, 'has_original_pagination', False),
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙØµÙˆÙ„ ÙˆØ§Ù„Ø£Ø¬Ø²Ø§Ø¡
                "chapters": [
                    {
                        "title": chapter.title,
                        "page_start": chapter.page_number,
                        "page_end": chapter.page_end,
                        "volume": chapter.volume_number if hasattr(chapter, 'volume_number') else None,
                        "order": chapter.order if hasattr(chapter, 'order') else None,
                        "level": chapter.level if hasattr(chapter, 'level') else None
                    } for chapter in book.chapters
                ] if hasattr(book, 'chapters') and book.chapters else [],
                
                "volumes": [
                    {
                        "volume_number": vol.number,
                        "page_start": vol.page_start,
                        "page_end": vol.page_end,
                        "title": vol.title if hasattr(vol, 'title') else f"Ø§Ù„Ø¬Ø²Ø¡ {vol.number}"
                    } for vol in book.volumes
                ] if hasattr(book, 'volumes') and book.volumes else [],
                
                "pages": [{"page_number": p.page_number, "content": p.content, "word_count": p.word_count} 
                         for p in book.pages],
                "extraction_metadata": {
                    "extraction_date": datetime.now().isoformat(),
                    "total_pages": len(book.pages),
                    "total_words": sum(p.word_count for p in book.pages),
                    "total_chapters": len(book.chapters) if hasattr(book, 'chapters') and book.chapters else 0,
                    "total_volumes": len(book.volumes) if hasattr(book, 'volumes') and book.volumes else 0,
                    "reliability_verified": True,
                    "scraper_version": "Ultra Reliable v3.1",
                    "extraction_method": "enhanced_reliability_with_structure"
                }
            }
            
            # Ø­ÙØ¸ Ø¨ØµÙŠØºØ© JSON Ù…Ø¶ØºÙˆØ·
            with gzip.open(f"{filepath}.gz", 'wt', encoding='utf-8') as f:
                json.dump(book_data, f, ensure_ascii=False, indent=2)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­ÙØ¸
            try:
                with gzip.open(f"{filepath}.gz", 'rt', encoding='utf-8') as f:
                    verified_data = json.load(f)
                    
                if len(verified_data['pages']) != len(book.pages):
                    raise Exception("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ø£ØµÙ„")
                    
                self.logger.info(f"ğŸ’¾ ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªØ­Ù‚Ù‚: {filepath}.gz")
                return f"{filepath}.gz"
                
            except Exception as ve:
                self.logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸: {str(ve)}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ÙØ¸: {str(e)}")
            return None

    def extract_book_ultra_reliable(self, book_id: str, max_pages: int = None, output_dir: str = None) -> Dict[str, Any]:
        """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
        
        start_time = time.time()
        result = {
            "book_id": book_id,
            "success": False,
            "error": None,
            "filepath": None,
            "stats": {},
            "attempts": 0
        }
        
        try:
            self.logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ù„Ù„ÙƒØªØ§Ø¨ {book_id}")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ù…Ø«Ù„
            config = self.get_optimal_config(max_pages)
            
            # Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
            book = self.extract_with_retry(book_id, max_pages, config)
            
            if not book:
                result["error"] = "ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¹Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª"
                return result
            
            # Ø§Ù„Ø­ÙØ¸ Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚
            output_dir = output_dir or "ultra_reliable_books"
            filepath = self.save_with_verification(book, output_dir)
            
            if not filepath:
                result["error"] = "ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù"
                return result
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¬Ø§Ø­
            elapsed_time = time.time() - start_time
            result.update({
                "success": True,
                "filepath": filepath,
                "stats": {
                    "pages_extracted": len(book.pages),
                    "total_words": sum(p.word_count for p in book.pages),
                    "extraction_time": elapsed_time,
                    "speed": len(book.pages) / elapsed_time if elapsed_time > 0 else 0,
                    "title": book.title
                }
            })
            
            self.logger.info(f"ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­: {len(book.pages)} ØµÙØ­Ø© ÙÙŠ {elapsed_time:.2f}Ø«")
            return result
            
        except Exception as e:
            error_msg = f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}"
            self.logger.error(f"ğŸ’¥ {error_msg}")
            self.logger.debug(traceback.format_exc())
            
            result["error"] = error_msg
            return result

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    
    print("ğŸ›¡ï¸ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©")
    print("=" * 50)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙØ³ØªØ®Ø±ÙØ¬
    scraper = UltraReliableScraper()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ ÙƒØªØ§Ø¨ ØµØºÙŠØ±
    result = scraper.extract_book_ultra_reliable("12106", max_pages=20)
    
    if result["success"]:
        stats = result["stats"]
        print(f"\nâœ… Ù†Ø¬Ø­ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!")
        print(f"ğŸ“š Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {stats['title']}")
        print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª: {stats['pages_extracted']}")
        print(f"ğŸ“ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {stats['total_words']:,}")
        print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {stats['extraction_time']:.2f}Ø«")
        print(f"ğŸï¸ Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['speed']:.2f} ØµÙØ­Ø©/Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ’¾ Ø§Ù„Ù…Ù„Ù: {result['filepath']}")
    else:
        print(f"\nâŒ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {result['error']}")

if __name__ == "__main__":
    main()
