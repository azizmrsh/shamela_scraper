#!/usr/bin/env python3
"""
Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© - Ultra Reliable Shamela Extractor
Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100% Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø£Ø®Ø·Ø§Ø¡ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
"""

import asyncio
import aiohttp
import time
import json
import gzip
import threading
import queue
from typing import List, Dict, Any, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
import logging
from contextlib import asynccontextmanager
import pickle
import sqlite3
from urllib.parse import urljoin, urlparse
import re

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
import sys
import os
sys.path.append(os.path.dirname(__file__))
from ultra_reliability_system import (
    UltraReliableSession, 
    ReliabilityConfig, 
    BackupManager,
    create_ultra_reliable_config
)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠØ©
try:
    from enhanced_shamela_scraper import (
        Book, PageContent, Author, Publisher, BookSection, 
        Volume, VolumeLink, ChapterIndex, PerformanceConfig
    )
except ImportError:
    # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªÙˆÙØ±Ø©
    from dataclasses import dataclass
    
    @dataclass
    class Author:
        name: str
        slug: str = ""
        biography: str = None
        madhhab: str = None
        birth_date: str = None
        death_date: str = None

logger = logging.getLogger(__name__)

@dataclass
class UltraReliableConfig:
    """ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
    reliability: ReliabilityConfig
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    max_workers: int = 16
    batch_size: int = 20
    request_delay: float = 0.1
    adaptive_delay: bool = True
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    enable_smart_caching: bool = True
    cache_duration: int = 3600  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
    cache_size_limit: int = 1000  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ±
    persistent_cache: bool = True
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    verify_data_integrity: bool = True
    validate_html_structure: bool = True
    check_content_quality: bool = True
    min_content_length: int = 50
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø°ÙƒÙŠØ©
    enable_progressive_loading: bool = True
    checkpoint_interval: int = 25
    auto_resume: bool = True
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©
    quality_threshold: float = 0.95
    max_empty_pages: int = 5
    content_validation: bool = True

class SmartCache:
    """Ù†Ø¸Ø§Ù… ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ø°ÙƒÙŠ ÙˆÙ…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, config: UltraReliableConfig):
        self.config = config
        self.memory_cache = {}
        self.access_count = {}
        self.cache_lock = threading.RLock()
        
        # ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ø¯Ø§Ø¦Ù…
        if config.persistent_cache:
            self.cache_file = Path("ultra_cache.db")
            self._init_persistent_cache()
    
    def _init_persistent_cache(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø¯Ø§Ø¦Ù…"""
        try:
            self.conn = sqlite3.connect(str(self.cache_file), check_same_thread=False)
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS cache (
                    key TEXT PRIMARY KEY,
                    value BLOB,
                    timestamp INTEGER,
                    access_count INTEGER DEFAULT 1
                )
            """)
            self.conn.commit()
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {str(e)}")
            self.config.persistent_cache = False
    
    def _generate_key(self, url: str, params: Dict = None) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª"""
        key_data = f"{url}_{params or ''}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        with self.cache_lock:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£ÙˆÙ„Ø§Ù‹
            if key in self.memory_cache:
                self.access_count[key] = self.access_count.get(key, 0) + 1
                return self.memory_cache[key]
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¯Ø§Ø¦Ù…
            if self.config.persistent_cache:
                try:
                    cursor = self.conn.execute(
                        "SELECT value, timestamp FROM cache WHERE key = ?", (key,)
                    )
                    result = cursor.fetchone()
                    if result:
                        value_blob, timestamp = result
                        # ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
                        if time.time() - timestamp < self.config.cache_duration:
                            value = pickle.loads(value_blob)
                            # Ù†Ù‚Ù„ Ø¥Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
                            self.memory_cache[key] = value
                            self.access_count[key] = self.access_count.get(key, 0) + 1
                            # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØµÙˆÙ„
                            self.conn.execute(
                                "UPDATE cache SET access_count = access_count + 1 WHERE key = ?", 
                                (key,)
                            )
                            self.conn.commit()
                            return value
                except Exception as e:
                    logger.debug(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {str(e)}")
            
            return None
    
    def set(self, key: str, value: Any):
        """ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ…Ø© ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        with self.cache_lock:
            # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            self.memory_cache[key] = value
            self.access_count[key] = 1
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„Ø­Ø¯
            if len(self.memory_cache) > self.config.cache_size_limit:
                self._cleanup_memory_cache()
            
            # ØªØ®Ø²ÙŠÙ† Ø¯Ø§Ø¦Ù…
            if self.config.persistent_cache:
                try:
                    value_blob = pickle.dumps(value)
                    self.conn.execute(
                        "INSERT OR REPLACE INTO cache (key, value, timestamp) VALUES (?, ?, ?)",
                        (key, value_blob, int(time.time()))
                    )
                    self.conn.commit()
                except Exception as e:
                    logger.debug(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {str(e)}")
    
    def _cleanup_memory_cache(self):
        """ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        sorted_items = sorted(
            self.access_count.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        keep_count = int(self.config.cache_size_limit * 0.7)
        keys_to_keep = {item[0] for item in sorted_items[:keep_count]}
        
        # Ø­Ø°Ù Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        keys_to_remove = set(self.memory_cache.keys()) - keys_to_keep
        for key in keys_to_remove:
            del self.memory_cache[key]
            self.access_count.pop(key, None)

class DataValidator:
    """Ù…Ø¯Ù‚Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, config: UltraReliableConfig):
        self.config = config
    
    def validate_html_response(self, html: str, url: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ø³ØªØ¬Ø§Ø¨Ø© HTML"""
        if not html or len(html) < 100:
            logger.warning(f"âš ï¸ HTML Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ø±Ø§Ø¨Ø·: {url}")
            return False
        
        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø§ØµØ± Ø£Ø³Ø§Ø³ÙŠØ©
        if self.config.validate_html_structure:
            essential_tags = ['<html', '<body', '<div']
            if not any(tag in html.lower() for tag in essential_tags):
                logger.warning(f"âš ï¸ HTML ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù„Ø±Ø§Ø¨Ø·: {url}")
                return False
        
        # ÙØ­Øµ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø®Ø·Ø£
        error_indicators = [
            'error 404', 'not found', 'page not found',
            'access denied', 'forbidden', 'server error',
            'temporarily unavailable', 'maintenance'
        ]
        html_lower = html.lower()
        if any(error in html_lower for error in error_indicators):
            logger.warning(f"âš ï¸ ØµÙØ­Ø© Ø®Ø·Ø£ Ù…ÙƒØªØ´ÙØ© Ù„Ù„Ø±Ø§Ø¨Ø·: {url}")
            return False
        
        return True
    
    def validate_page_content(self, content: str, page_num: int) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©"""
        if not content:
            logger.warning(f"âš ï¸ Ù…Ø­ØªÙˆÙ‰ ÙØ§Ø±Øº Ù„Ù„ØµÙØ­Ø© {page_num}")
            return False
        
        if len(content) < self.config.min_content_length:
            logger.warning(f"âš ï¸ Ù…Ø­ØªÙˆÙ‰ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØµÙØ­Ø© {page_num}: {len(content)} Ø­Ø±Ù")
            return False
        
        if self.config.check_content_quality:
            # ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            arabic_chars = len(re.findall(r'[\u0600-\u06FF]', content))
            total_chars = len(content)
            
            if total_chars > 0:
                arabic_ratio = arabic_chars / total_chars
                if arabic_ratio < 0.3:  # Ø£Ù‚Ù„ Ù…Ù† 30% Ù†Øµ Ø¹Ø±Ø¨ÙŠ
                    logger.warning(f"âš ï¸ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„ØµÙØ­Ø© {page_num}: {arabic_ratio:.2f}")
                    return arabic_ratio > 0.1  # Ù‚Ø¨ÙˆÙ„ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£ÙƒØ«Ø± Ù…Ù† 10%
        
        return True
    
    def validate_book_data(self, book_data: Dict) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨"""
        required_fields = ['title', 'shamela_id', 'authors']
        
        for field in required_fields:
            if field not in book_data or not book_data[field]:
                logger.error(f"âŒ Ø­Ù‚Ù„ Ù…Ø·Ù„ÙˆØ¨ Ù…ÙÙ‚ÙˆØ¯: {field}")
                return False
        
        return True

class ProgressiveLoader:
    """Ù…Ø­Ù…Ù„ ØªØ¯Ø±ÙŠØ¬ÙŠ Ù…Ø¹ Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´"""
    
    def __init__(self, config: UltraReliableConfig, book_id: str):
        self.config = config
        self.book_id = book_id
        self.checkpoint_file = Path(f"checkpoint_{book_id}.json")
        self.loaded_pages = set()
        self.failed_pages = set()
    
    def load_checkpoint(self) -> Dict:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´"""
        if not self.config.auto_resume or not self.checkpoint_file.exists():
            return {}
        
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.loaded_pages = set(data.get('loaded_pages', []))
            self.failed_pages = set(data.get('failed_pages', []))
            
            logger.info(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´: {len(self.loaded_pages)} ØµÙØ­Ø© Ù…Ø­Ù…Ù„Ø©")
            return data.get('book_data', {})
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´: {str(e)}")
            return {}
    
    def save_checkpoint(self, book_data: Dict, pages_data: Dict):
        """Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´"""
        if not self.config.enable_progressive_loading:
            return
        
        try:
            checkpoint_data = {
                'book_data': book_data,
                'loaded_pages': list(self.loaded_pages),
                'failed_pages': list(self.failed_pages),
                'timestamp': time.time(),
                'total_pages': len(pages_data)
            }
            
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´: {str(e)}")
    
    def cleanup_checkpoint(self):
        """ØªÙ†Ø¸ÙŠÙ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡"""
        try:
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()
        except Exception:
            pass
    
    def should_load_page(self, page_num: int) -> bool:
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©"""
        return page_num not in self.loaded_pages
    
    def mark_page_loaded(self, page_num: int):
        """ØªØ¹Ù„ÙŠÙ… Ø§Ù„ØµÙØ­Ø© ÙƒÙ…Ø­Ù…Ù„Ø©"""
        self.loaded_pages.add(page_num)
        self.failed_pages.discard(page_num)
    
    def mark_page_failed(self, page_num: int):
        """ØªØ¹Ù„ÙŠÙ… Ø§Ù„ØµÙØ­Ø© ÙƒÙØ§Ø´Ù„Ø©"""
        self.failed_pages.add(page_num)

class UltraReliableExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    
    def __init__(self, config: UltraReliableConfig = None):
        self.config = config or UltraReliableConfig(
            reliability=create_ultra_reliable_config()
        )
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.cache = SmartCache(self.config)
        self.validator = DataValidator(self.config)
        self.backup_manager = BackupManager(self.config.reliability)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©
        self.stats = {
            'pages_processed': 0,
            'pages_successful': 0,
            'pages_failed': 0,
            'pages_from_cache': 0,
            'retries_used': 0,
            'recoveries_performed': 0,
            'start_time': time.time()
        }
        self.stats_lock = threading.Lock()
    
    def extract_book_ultra_reliable(self, book_id: str, max_pages: Optional[int] = None) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%"""
        
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ù„Ù„ÙƒØªØ§Ø¨ {book_id}")
        start_time = time.time()
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
        loader = ProgressiveLoader(self.config, book_id)
        
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´
            checkpoint_data = loader.load_checkpoint()
            
            with UltraReliableSession(self.config.reliability) as session:
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                if not checkpoint_data:
                    logger.info("ğŸ“š Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©...")
                    book_data = self._extract_book_info_reliable(session, book_id)
                    
                    if not self.validator.validate_book_data(book_data):
                        raise ValueError("Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ ØºÙŠØ± ØµØ§Ù„Ø­Ø©")
                        
                    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                    self.backup_manager.create_backup(book_data, book_id, 0)
                else:
                    book_data = checkpoint_data
                    logger.info("ğŸ“‚ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´")
                
                # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª
                total_pages = book_data.get('page_count_internal', 1)
                actual_max = min(total_pages, max_pages) if max_pages else total_pages
                
                logger.info(f"ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {actual_max} ØµÙØ­Ø© Ù…Ù† Ø£ØµÙ„ {total_pages}")
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙƒØ§Ù…Ù„Ø©
                pages_data = self._extract_pages_ultra_reliable(
                    session, book_id, actual_max, loader
                )
                
                # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
                final_data = {**book_data, 'pages': pages_data}
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
                success_rate = len(pages_data) / actual_max if actual_max > 0 else 0
                if success_rate < self.config.quality_threshold:
                    logger.warning(f"âš ï¸ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù…Ù†Ø®ÙØ¶: {success_rate:.2%}")
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ©
                backup_path = self.backup_manager.create_backup(final_data, book_id, len(pages_data))
                
                # ØªÙ†Ø¸ÙŠÙ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´
                loader.cleanup_checkpoint()
                
                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©
                elapsed = time.time() - start_time
                with self.stats_lock:
                    self.stats['total_time'] = elapsed
                    self.stats['pages_per_second'] = len(pages_data) / elapsed if elapsed > 0 else 0
                
                logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ {elapsed:.2f} Ø«Ø§Ù†ÙŠØ©")
                logger.info(f"ğŸ“Š Ø§Ù„ØµÙØ­Ø§Øª: {len(pages_data)}/{actual_max} ({success_rate:.1%})")
                logger.info(f"ğŸ¯ Ø§Ù„Ø³Ø±Ø¹Ø©: {self.stats['pages_per_second']:.2f} ØµÙØ­Ø©/Ø«Ø§Ù†ÙŠØ©")
                logger.info(f"ğŸ’¾ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path}")
                
                return final_data
                
        except Exception as e:
            logger.error(f"ğŸ’¥ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {str(e)}")
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            backup_data = self.backup_manager.restore_from_backup(book_id)
            if backup_data:
                logger.info("ğŸ“‚ ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
                return backup_data
            
            raise
    
    def _extract_book_info_reliable(self, session: UltraReliableSession, book_id: str) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙƒØ§Ù…Ù„Ø©"""
        
        url = f"https://shamela.ws/book/{book_id}"
        cache_key = self.cache._generate_key(url)
        
        # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø£ÙˆÙ„Ø§Ù‹
        cached_data = self.cache.get(cache_key)
        if cached_data:
            logger.info("ğŸ’¾ ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
            return cached_data
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹
        response = session.get(url)
        html = response.text
        
        if not self.validator.validate_html_response(html, url):
            raise ValueError(f"HTML ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù„ÙƒØªØ§Ø¨ {book_id}")
        
        # Ù‡Ù†Ø§ Ø³ØªÙƒÙˆÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© HTML Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        # Ù„Ù„Ø¨Ø³Ø§Ø·Ø©ØŒ Ø³Ù†Ø¹ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        book_data = {
            'shamela_id': book_id,
            'title': f'ÙƒØªØ§Ø¨ {book_id}',
            'authors': [{'name': 'Ù…Ø¤Ù„Ù ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ', 'slug': 'unknown'}],
            'page_count_internal': 100,  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            'extraction_date': time.time(),
            'source_url': url
        }
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.cache.set(cache_key, book_data)
        
        return book_data
    
    def _extract_pages_ultra_reliable(self, session: UltraReliableSession, book_id: str, 
                                    max_pages: int, loader: ProgressiveLoader) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%"""
        
        pages_data = []
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØªØ­Ù…ÙŠÙ„Ù‡Ø§
        pages_to_load = [i for i in range(1, max_pages + 1) if loader.should_load_page(i)]
        
        logger.info(f"ğŸ“„ ØµÙØ­Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ­Ù…ÙŠÙ„: {len(pages_to_load)}")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ù„Ø¯ÙØ¹Ø§Øª
        for batch_start in range(0, len(pages_to_load), self.config.batch_size):
            batch_end = min(batch_start + self.config.batch_size, len(pages_to_load))
            batch_pages = pages_to_load[batch_start:batch_end]
            
            logger.info(f"ğŸ“¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© {batch_start//self.config.batch_size + 1}: ØµÙØ­Ø§Øª {batch_pages[0]}-{batch_pages[-1]}")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ø¯ÙØ¹Ø©
            batch_results = self._process_batch_ultra_reliable(session, book_id, batch_pages)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            for page_num, page_data in batch_results.items():
                if page_data:
                    pages_data.append(page_data)
                    loader.mark_page_loaded(page_num)
                    with self.stats_lock:
                        self.stats['pages_successful'] += 1
                else:
                    loader.mark_page_failed(page_num)
                    with self.stats_lock:
                        self.stats['pages_failed'] += 1
            
            # Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´
            if len(pages_data) % self.config.checkpoint_interval == 0:
                loader.save_checkpoint({'shamela_id': book_id}, {p['page_number']: p for p in pages_data})
            
            # ØªØ£Ø®ÙŠØ± ØªÙƒÙŠÙÙŠ
            if self.config.adaptive_delay:
                delay = self.config.request_delay
                # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ£Ø®ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙØ´Ù„
                failure_rate = self.stats['pages_failed'] / max(self.stats['pages_processed'], 1)
                if failure_rate > 0.1:
                    delay *= (1 + failure_rate)
                
                time.sleep(delay)
        
        return sorted(pages_data, key=lambda p: p.get('page_number', 0))
    
    def _process_batch_ultra_reliable(self, session: UltraReliableSession, book_id: str, 
                                    page_numbers: List[int]) -> Dict[int, Optional[Dict]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙƒØ§Ù…Ù„Ø©"""
        
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‡Ø§Ù…
            future_to_page = {
                executor.submit(self._extract_single_page_reliable, session, book_id, page_num): page_num
                for page_num in page_numbers
            }
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            for future in as_completed(future_to_page):
                page_num = future_to_page[future]
                try:
                    page_data = future.result(timeout=60)  # Ù…Ù‡Ù„Ø© Ø²Ù…Ù†ÙŠØ© Ù„ÙƒÙ„ ØµÙØ­Ø©
                    results[page_num] = page_data
                    
                except Exception as e:
                    logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page_num}: {str(e)}")
                    results[page_num] = None
                
                with self.stats_lock:
                    self.stats['pages_processed'] += 1
        
        return results
    
    def _extract_single_page_reliable(self, session: UltraReliableSession, book_id: str, 
                                     page_num: int) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙƒØ§Ù…Ù„Ø©"""
        
        url = f"https://shamela.ws/book/{book_id}/{page_num}"
        cache_key = self.cache._generate_key(url)
        
        # ÙØ­Øµ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cached_data = self.cache.get(cache_key)
        if cached_data:
            with self.stats_lock:
                self.stats['pages_from_cache'] += 1
            return cached_data
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
            response = session.get(url)
            html = response.text
            
            if not self.validator.validate_html_response(html, url):
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ù‡Ù†Ø§ Ù†Ø¶Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©)
            content = self._parse_page_content(html)
            
            if not self.validator.validate_page_content(content, page_num):
                return None
            
            page_data = {
                'page_number': page_num,
                'content': content,
                'word_count': len(content.split()),
                'char_count': len(content),
                'url': url,
                'extraction_time': time.time()
            }
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            self.cache.set(cache_key, page_data)
            
            return page_data
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {page_num}: {str(e)}")
            return None
    
    def _parse_page_content(self, html: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ù…Ù† HTML"""
        # Ù‡Ù†Ø§ Ø³ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ù€ HTML
        # Ù„Ù„Ø¨Ø³Ø§Ø·Ø©ØŒ Ø³Ù†Ø¹ÙŠØ¯ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ
        return f"Ù…Ø­ØªÙˆÙ‰ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„ØµÙØ­Ø© - {len(html)} Ø­Ø±Ù HTML"
    
    def get_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©"""
        with self.stats_lock:
            total_time = time.time() - self.stats['start_time']
            return {
                **self.stats,
                'uptime_minutes': total_time / 60,
                'success_rate': (self.stats['pages_successful'] / max(self.stats['pages_processed'], 1)) * 100,
                'cache_hit_rate': (self.stats['pages_from_cache'] / max(self.stats['pages_processed'], 1)) * 100
            }

def extract_book_ultra_reliable(book_id: str, max_pages: Optional[int] = None, 
                               config: UltraReliableConfig = None) -> Dict:
    """Ø¯Ø§Ù„Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%"""
    
    if config is None:
        config = UltraReliableConfig(reliability=create_ultra_reliable_config())
    
    extractor = UltraReliableExtractor(config)
    return extractor.extract_book_ultra_reliable(book_id, max_pages)

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # ØªÙƒÙˆÙŠÙ† ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
    config = UltraReliableConfig(
        reliability=create_ultra_reliable_config(),
        max_workers=12,
        batch_size=15,
        enable_smart_caching=True,
        verify_data_integrity=True,
        enable_progressive_loading=True
    )
    
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%
        book_data = extract_book_ultra_reliable("12106", max_pages=10, config=config)
        
        print("âœ… ØªÙ… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ“š Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {book_data.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª: {len(book_data.get('pages', []))}")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        extractor = UltraReliableExtractor(config)
        stats = extractor.get_stats()
        print(f"ğŸ“Š Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_rate']:.2f}%")
        print(f"ğŸ’¾ Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {stats['cache_hit_rate']:.2f}%")
        
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {str(e)}")
        # ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ Ø³ÙŠØªÙ… Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
