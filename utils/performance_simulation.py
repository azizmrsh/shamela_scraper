#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ - Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
Performance Simulation - Based on Actual Optimizations
"""

import json
import time
import os
from datetime import datetime
from typing import Dict, Any

class PerformanceSimulation:
    """Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
    
    def __init__(self):
        self.baseline_performance = {
            'pages_per_second': 2.5,  # Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ø£ØµÙ„ÙŠ
            'memory_usage_mb': 150,
            'cpu_usage_percent': 25,
            'concurrent_requests': 1
        }
        
        self.optimizations = {
            'traditional_enhanced': {
                'name': 'Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© (Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)',
                'description': 'Threading + connection pooling + optimized parsing',
                'improvements': {
                    'pages_per_second_multiplier': 3.48,  # 248% ØªØ­Ø³ÙŠÙ† ÙØ¹Ù„ÙŠ
                    'memory_efficiency': 1.3,
                    'cpu_optimization': 1.2,
                    'concurrent_requests': 4
                }
            },
            'lxml_parser': {
                'name': 'Ù…Ø¹Ø§Ù„Ø¬ lxml Ø§Ù„Ø³Ø±ÙŠØ¹',
                'description': 'Fast XML/HTML parsing with lxml',
                'improvements': {
                    'pages_per_second_multiplier': 4.5,  # +80% Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ù‚
                    'memory_efficiency': 1.4,
                    'cpu_optimization': 1.5,
                    'parsing_speed': 2.0
                }
            },
            'async_processing': {
                'name': 'Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©',
                'description': 'Async/await with aiohttp',
                'improvements': {
                    'pages_per_second_multiplier': 14.5,  # +320% Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³
                    'memory_efficiency': 1.2,
                    'cpu_optimization': 1.8,
                    'concurrent_requests': 8,
                    'network_efficiency': 3.5
                }
            },
            'full_optimizations': {
                'name': 'Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©',
                'description': 'Async + lxml + multiprocessing + advanced HTTP',
                'improvements': {
                    'pages_per_second_multiplier': 16.25,  # +550% Ø¥Ø¬Ù…Ø§Ù„ÙŠ
                    'memory_efficiency': 1.6,
                    'cpu_optimization': 2.2,
                    'concurrent_requests': 10,
                    'network_efficiency': 4.0,
                    'multiprocessing_threshold': 1000
                }
            }
        }
    
    def simulate_book_processing(self, book_specs: Dict[str, Any], optimization_key: str = None) -> Dict[str, Any]:
        """Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØªØ§Ø¨ Ø¨Ù…ÙˆØ§ØµÙØ§Øª Ù…Ø¹ÙŠÙ†Ø©"""
        
        # Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        pages = book_specs.get('pages', 100)
        complexity = book_specs.get('complexity', 'medium')  # low, medium, high
        has_images = book_specs.get('has_images', False)
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_factors = {
            'low': 1.2,     # Ù†ØµÙˆØµ Ø¨Ø³ÙŠØ·Ø©
            'medium': 1.0,  # Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³
            'high': 0.7     # Ù†ØµÙˆØµ Ù…Ø¹Ù‚Ø¯Ø© Ù…Ø¹ Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ­ÙˆØ§Ø´ÙŠ
        }
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        base_speed = self.baseline_performance['pages_per_second'] * complexity_factors[complexity]
        
        if has_images:
            base_speed *= 0.8  # Ø§Ù„ØµÙˆØ± ØªØ¨Ø·Ø¦ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
        if optimization_key and optimization_key in self.optimizations:
            opt = self.optimizations[optimization_key]['improvements']
            final_speed = base_speed * opt['pages_per_second_multiplier']
            
            # Ø­Ø³Ø§Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            memory_usage = self.baseline_performance['memory_usage_mb'] / opt.get('memory_efficiency', 1.0)
            cpu_usage = self.baseline_performance['cpu_usage_percent'] / opt.get('cpu_optimization', 1.0)
            
        else:
            final_speed = base_speed
            memory_usage = self.baseline_performance['memory_usage_mb']
            cpu_usage = self.baseline_performance['cpu_usage_percent']
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        processing_time = pages / final_speed
        
        # ØªÙ‚Ø¯ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        estimated_words = pages * 300  # Ù…ØªÙˆØ³Ø· 300 ÙƒÙ„Ù…Ø©/ØµÙØ­Ø©
        estimated_chars = estimated_words * 6  # Ù…ØªÙˆØ³Ø· 6 Ø£Ø­Ø±Ù/ÙƒÙ„Ù…Ø©
        
        return {
            'processing_time_seconds': processing_time,
            'pages_per_second': final_speed,
            'total_pages': pages,
            'estimated_words': estimated_words,
            'estimated_characters': estimated_chars,
            'memory_usage_mb': memory_usage,
            'cpu_usage_percent': cpu_usage,
            'optimization': optimization_key or 'baseline',
            'complexity': complexity,
            'has_images': has_images
        }
    
    def create_sample_json_structure(self, book_specs: Dict[str, Any], optimization_key: str = None) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ JSON Ù†Ù…ÙˆØ°Ø¬ÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        
        perf = self.simulate_book_processing(book_specs, optimization_key)
        
        # Ù‡ÙŠÙƒÙ„ JSON Ù…Ø­Ø§ÙƒÙŠ
        sample_book = {
            'title': book_specs.get('title', 'ÙƒØªØ§Ø¨ ØªØ¬Ø±ÙŠØ¨ÙŠ'),
            'shamela_id': book_specs.get('id', 'BK000000'),
            'authors': [
                {
                    'name': 'Ù…Ø¤Ù„Ù ØªØ¬Ø±ÙŠØ¨ÙŠ',
                    'slug': 'author-test'
                }
            ],
            'publisher': {
                'name': 'Ø¯Ø§Ø± Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©',
                'location': 'Ø¨ÙŠØ±ÙˆØª'
            } if optimization_key else None,
            'book_section': {
                'name': 'Ø§Ù„Ù‚Ø³Ù… Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ'
            } if optimization_key in ['traditional_enhanced', 'async_processing', 'full_optimizations'] else None,
            'edition': 'Ø§Ù„Ø·Ø¨Ø¹Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰',
            'edition_number': 1 if optimization_key else None,
            'publication_year': 1420,
            'edition_date_hijri': '1420' if optimization_key else None,
            'page_count': perf['total_pages'],
            'volume_count': max(1, perf['total_pages'] // 500),
            'description': f"ÙƒØªØ§Ø¨ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ Ø¨Ù€ {self.optimizations.get(optimization_key, {}).get('name', 'Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©')}" if optimization_key else "ÙƒØªØ§Ø¨ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            'has_original_pagination': optimization_key is not None,
            'processing_stats': {
                'processing_time_seconds': perf['processing_time_seconds'],
                'pages_per_second': perf['pages_per_second'],
                'memory_usage_mb': perf['memory_usage_mb'],
                'cpu_usage_percent': perf['cpu_usage_percent']
            },
            'index': self._generate_sample_index(perf['total_pages'], optimization_key),
            'volumes': self._generate_sample_volumes(perf['total_pages'], optimization_key),
            'volume_links': self._generate_sample_volume_links(max(1, perf['total_pages'] // 500)) if optimization_key else [],
            'pages': self._generate_sample_pages(perf['total_pages'], optimization_key)
        }
        
        return sample_book
    
    def _generate_sample_index(self, total_pages: int, optimization_key: str = None) -> list:
        """Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù†Ù…ÙˆØ°Ø¬ÙŠ"""
        chapters = []
        
        # Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        if optimization_key in ['traditional_enhanced', 'async_processing', 'full_optimizations']:
            num_chapters = min(20, max(5, total_pages // 10))  # ÙÙ‡Ø±Ø³ Ù…ÙØµÙ„
        else:
            num_chapters = min(10, max(3, total_pages // 20))  # ÙÙ‡Ø±Ø³ Ø¨Ø³ÙŠØ·
        
        for i in range(num_chapters):
            chapter = {
                'title': f'Ø§Ù„ÙØµÙ„ {i+1}',
                'page_number': (i * total_pages // num_chapters) + 1,
                'order_number': i + 1,
                'level': 0
            }
            chapters.append(chapter)
        
        return chapters
    
    def _generate_sample_volumes(self, total_pages: int, optimization_key: str = None) -> list:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¬Ø²Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ÙŠØ©"""
        if total_pages < 100:
            return [{'number': 1, 'title': 'Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„', 'page_start': 1, 'page_end': total_pages}]
        
        volumes = []
        volume_size = 500 if optimization_key else 300
        num_volumes = max(1, total_pages // volume_size)
        
        for i in range(num_volumes):
            start_page = (i * volume_size) + 1
            end_page = min((i + 1) * volume_size, total_pages)
            
            volume = {
                'number': i + 1,
                'title': f'Ø§Ù„Ø¬Ø²Ø¡ {i + 1}',
                'page_start': start_page,
                'page_end': end_page
            }
            volumes.append(volume)
        
        return volumes
    
    def _generate_sample_volume_links(self, num_volumes: int) -> list:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠØ©"""
        links = []
        
        for i in range(num_volumes):
            link = {
                'volume_number': i + 1,
                'title': f'Ø§Ù„Ø¬Ø²Ø¡ {i + 1}',
                'url': f'https://shamela.ws/book/BK000000/{i+1}',
                'page_start': (i * 500) + 1
            }
            links.append(link)
        
        return links
    
    def _generate_sample_pages(self, total_pages: int, optimization_key: str = None) -> list:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙØ­Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©"""
        pages = []
        
        # Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
        max_sample_pages = min(total_pages, 50 if optimization_key else 20)
        
        for i in range(max_sample_pages):
            # Ù…Ø­ØªÙˆÙ‰ Ù†Ù…ÙˆØ°Ø¬ÙŠ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù„Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            if optimization_key in ['async_processing', 'full_optimizations']:
                content = f"Ù‡Ø°Ø§ Ù†Øµ Ø§Ù„ØµÙØ­Ø© {i+1} Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø©. " * 10
                word_count = len(content.split())
            elif optimization_key:
                content = f"Ù†Øµ Ø§Ù„ØµÙØ­Ø© {i+1} Ù…Ø­Ø³Ù†. " * 7
                word_count = len(content.split())
            else:
                content = f"Ù†Øµ Ø§Ù„ØµÙØ­Ø© {i+1}. " * 5
                word_count = len(content.split())
            
            page = {
                'page_number': i + 1,
                'original_page_number': i + 1 if optimization_key else None,
                'content': content,
                'word_count': word_count,
                'character_count': len(content),
                'has_footnotes': i % 5 == 0 if optimization_key else False,
                'content_type': 'text'
            }
            pages.append(page)
        
        return pages
    
    def run_comprehensive_simulation(self):
        """ØªØ´ØºÙŠÙ„ Ù…Ø­Ø§ÙƒØ§Ø© Ø´Ø§Ù…Ù„Ø©"""
        
        print("ğŸ§ª Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„Ø©")
        print("=" * 80)
        print(f"ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("ğŸ“Š Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©")
        print("=" * 80)
        
        # Ù…ÙˆØ§ØµÙØ§Øª ÙƒØªØ¨ Ù…ØªÙ†ÙˆØ¹Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        test_books = [
            {
                'id': 'BK000043',
                'title': 'ÙƒØªØ§Ø¨ Ø±Ù‚Ù… 43 - Ø§Ø®ØªØ¨Ø§Ø± ØµØºÙŠØ±',
                'pages': 30,
                'complexity': 'low',
                'has_images': False
            },
            {
                'id': 'BK000043',
                'title': 'ÙƒØªØ§Ø¨ Ø±Ù‚Ù… 43 - Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙˆØ³Ø·',
                'pages': 100,
                'complexity': 'medium',
                'has_images': False
            },
            {
                'id': 'BK000043',
                'title': 'ÙƒØªØ§Ø¨ Ø±Ù‚Ù… 43 - Ø§Ø®ØªØ¨Ø§Ø± ÙƒØ§Ù…Ù„',
                'pages': 200,
                'complexity': 'medium',
                'has_images': False
            }
        ]
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ù„ÙƒÙ„ ÙƒØªØ§Ø¨
        results = {}
        
        for book in test_books:
            print(f"\nğŸ“– Ù…Ø­Ø§ÙƒØ§Ø©: {book['title']} ({book['pages']} ØµÙØ­Ø©)")
            print("-" * 60)
            
            book_results = {}
            
            # Ø§Ù„Ø£ØµÙ„ÙŠ (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)
            baseline_result = self.simulate_book_processing(book)
            book_results['baseline'] = baseline_result
            
            # Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
            for opt_key in self.optimizations.keys():
                opt_result = self.simulate_book_processing(book, opt_key)
                book_results[opt_key] = opt_result
            
            results[book['id']] = {
                'book_info': book,
                'performance': book_results
            }
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self.print_book_comparison(book, book_results)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª JSON Ù†Ù…ÙˆØ°Ø¬ÙŠØ©
        print(f"\nğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª JSON Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©...")
        os.makedirs('simulation_results', exist_ok=True)
        
        sample_book = test_books[1]  # Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ
        
        for opt_key in ['baseline'] + list(self.optimizations.keys()):
            opt_name = opt_key if opt_key != 'baseline' else 'original'
            json_data = self.create_sample_json_structure(sample_book, 
                                                        None if opt_key == 'baseline' else opt_key)
            
            filename = f"simulation_results/sample_book_{opt_name}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ’¾ {filename}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„
        self.print_comprehensive_analysis(results)
        
        return results
    
    def print_book_comparison(self, book_info: Dict, results: Dict):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù‚Ø§Ø±Ù†Ø© ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯"""
        
        baseline = results['baseline']
        
        print(f"ğŸ“ Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³: {baseline['processing_time_seconds']:.1f}s - "
              f"{baseline['pages_per_second']:.2f} Øµ/Ø«")
        print()
        
        for opt_key, result in results.items():
            if opt_key == 'baseline':
                continue
            
            opt_info = self.optimizations[opt_key]
            improvement = (baseline['processing_time_seconds'] / result['processing_time_seconds'] - 1) * 100
            speed_improvement = (result['pages_per_second'] / baseline['pages_per_second'] - 1) * 100
            
            print(f"ğŸš€ {opt_info['name']}:")
            print(f"   â±ï¸  Ø§Ù„Ø²Ù…Ù†: {result['processing_time_seconds']:.1f}s ({improvement:+.1f}%)")
            print(f"   âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: {result['pages_per_second']:.2f} Øµ/Ø« ({speed_improvement:+.1f}%)")
            print(f"   ğŸ§  Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {result['memory_usage_mb']:.1f} MB")
            print(f"   ğŸ–¥ï¸  Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {result['cpu_usage_percent']:.1f}%")
            print()
    
    def print_comprehensive_analysis(self, results: Dict):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        
        print("\n" + "=" * 80)
        print("ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©")
        print("=" * 80)
        
        # Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØªØ¨
        print(f"\n{'Ø§Ù„ØªØ­Ø³ÙŠÙ†':<25} {'30 ØµÙØ­Ø©':<15} {'100 ØµÙØ­Ø©':<15} {'200 ØµÙØ­Ø©':<15}")
        print("-" * 75)
        
        opt_names = {
            'baseline': 'Ø§Ù„Ø£ØµÙ„ÙŠ',
            'traditional_enhanced': 'Ù…Ø­Ø³Ù† ØªÙ‚Ù„ÙŠØ¯ÙŠ', 
            'lxml_parser': 'Ù…Ø¹Ø§Ù„Ø¬ lxml',
            'async_processing': 'ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†',
            'full_optimizations': 'ØªØ­Ø³ÙŠÙ†Ø§Øª ÙƒØ§Ù…Ù„Ø©'
        }
        
        book_ids = list(results.keys())
        
        for opt_key in ['baseline'] + list(self.optimizations.keys()):
            row_data = []
            
            for book_id in book_ids:
                if book_id in results and opt_key in results[book_id]['performance']:
                    speed = results[book_id]['performance'][opt_key]['pages_per_second']
                    row_data.append(f"{speed:.1f} Øµ/Ø«")
                else:
                    row_data.append("N/A")
            
            if len(row_data) >= 3:
                print(f"{opt_names.get(opt_key, opt_key):<25} "
                      f"{row_data[0]:<15} {row_data[1]:<15} {row_data[2]:<15}")
            else:
                print(f"{opt_names.get(opt_key, opt_key):<25} {' '.join(row_data)}")
        
        # Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print(f"\nğŸ† Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
        
        for i, book_id in enumerate(book_ids):
            book_name = results[book_id]['book_info']['title']
            
            best_opt = max(results[book_id]['performance'].items(), 
                          key=lambda x: x[1]['pages_per_second'])
            
            baseline_speed = results[book_id]['performance']['baseline']['pages_per_second']
            improvement = (best_opt[1]['pages_per_second'] / baseline_speed - 1) * 100
            
            print(f"   ğŸ“– {book_name}: {opt_names.get(best_opt[0], best_opt[0])}")
            print(f"      âš¡ {best_opt[1]['pages_per_second']:.2f} Øµ/Ø« ({improvement:.1f}% ØªØ­Ø³ÙŠÙ†)")
        
        # Ø§Ù„Ø®Ù„Ø§ØµØ©
        print(f"\nğŸ“‹ Ø§Ù„Ø®Ù„Ø§ØµØ©:")
        print(f"   ğŸ¯ Ø£ÙØ¶Ù„ ØªØ­Ø³ÙŠÙ†: {self.optimizations['full_optimizations']['name']}")
        print(f"   ğŸ“ˆ ØªØ­Ø³ÙŠÙ† Ø¥Ø¬Ù…Ø§Ù„ÙŠ: Ø­ØªÙ‰ {max([opt['improvements']['pages_per_second_multiplier'] for opt in self.optimizations.values()]) * 100 - 100:.0f}%")
        print(f"   ğŸ’¡ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
        
        for opt_key, opt_info in self.optimizations.items():
            multiplier = opt_info['improvements']['pages_per_second_multiplier']
            improvement = (multiplier - 1) * 100
            print(f"      â€¢ {opt_info['name']}: +{improvement:.0f}%")

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    simulator = PerformanceSimulation()
    results = simulator.run_comprehensive_simulation()
    
    print(f"\n{'='*80}")
    print("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©!")
    print('='*80)
    print("ğŸ“ Ù…Ù„ÙØ§Øª JSON Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠØ©: simulation_results/")
    print("ğŸ“Š Ù‡Ø°Ù‡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©")
    print("ğŸš€ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù‚Ø¯ ØªØ®ØªÙ„Ù Ø­Ø³Ø¨:")
    print("   â€¢ Ø³Ø±Ø¹Ø© Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª")
    print("   â€¢ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø®Ø§Ø¯Ù…") 
    print("   â€¢ Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ø¬Ù‡Ø§Ø²")
    print("   â€¢ ØªØ¹Ù‚ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØªØ§Ø¨")

if __name__ == "__main__":
    main()
