#!/usr/bin/env python3
"""
Ultra Reliable Enhanced Runner - Ø§Ù„Ù…ÙØ´ØºÙ„ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø£ÙˆØ§Ù…Ø± Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%
"""

import argparse
import sys
import os
import json
import gzip
from datetime import datetime
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from ultra_reliable_scraper import UltraReliableScraper, ReliabilityConfig
    # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        from enhanced_database_manager import save_enhanced_json_to_database
    except ImportError:
        print("âš ï¸ ÙˆØ­Ø¯Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© - Ø³ÙŠØªÙ… Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø·")
        save_enhanced_json_to_database = None
except ImportError as e:
    print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯: {e}")
    print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù ultra_reliable_scraper.py")
    sys.exit(1)

def print_header():
    """Ø·Ø¨Ø§Ø¹Ø© Ø±Ø£Ø³ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
    print("=" * 70)
    print("ğŸ›¡ï¸ Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©")
    print("Ultra Reliable Enhanced Shamela Scraper")
    print("âœ… Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100% - Ø¨Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡ - Ø¯Ù‚Ø© ÙƒØ§Ù…Ù„Ø©")
    print("=" * 70)
    print()

def print_separator():
    """Ø·Ø¨Ø§Ø¹Ø© ÙØ§ØµÙ„"""
    print("-" * 70)

def extract_book_command(args):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ Ù…Ø¹ Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%"""
    print_header()
    
    print(f"ğŸ¯ Ø§Ù„Ù…Ù‡Ù…Ø©: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ {args.book_id}")
    print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {args.max_pages if args.max_pages else 'Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª'}")
    print_separator()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
    reliability_config = ReliabilityConfig()
    reliability_config.max_retries = 10  # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    reliability_config.retry_delay = 3.0  # ØªØ£Ø®ÙŠØ± Ø£Ø·ÙˆÙ„
    reliability_config.verify_extraction = True  # ØªØ­Ù‚Ù‚ Ø´Ø§Ù…Ù„
    reliability_config.detailed_logging = True  # Ø³Ø¬Ù„Ø§Øª Ù…ÙØµÙ„Ø©
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙØ³ØªØ®Ø±ÙØ¬
    scraper = UltraReliableScraper(reliability_config)
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    result = scraper.extract_book_ultra_reliable(
        book_id=args.book_id,
        max_pages=args.max_pages,
        output_dir=args.output_dir or "ultra_reliable_books"
    )
    
    print_separator()
    
    if result["success"]:
        stats = result["stats"]
        print("ğŸ‰ Ù†Ø¬Ø­ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%!")
        print(f"ğŸ“š Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {stats['title']}")
        print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {stats['pages_extracted']}")
        print(f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {stats['total_words']:,}")
        print(f"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {stats['extraction_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸï¸ Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['speed']:.2f} ØµÙØ­Ø©/Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ’¾ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸: {result['filepath']}")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù„Ù
        print(f"\nğŸ” Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù„Ù...")
        quality_report = verify_file_quality(result['filepath'])
        print_quality_report(quality_report)
        
    else:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {result['error']}")
        print("ğŸ’¡ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨")
        return 1
    
    print_separator()
    print("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
    return 0

def verify_file_quality(filepath):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸"""
    try:
        with gzip.open(filepath, 'rt', encoding='utf-8') as f:
            data = json.load(f)
        
        report = {
            'file_readable': True,
            'total_pages': len(data.get('pages', [])),
            'total_words': sum(p.get('word_count', 0) for p in data.get('pages', [])),
            'empty_pages': 0,
            'arabic_content': 0,
            'avg_words_per_page': 0,
            'quality_score': 0
        }
        
        # ÙØ­Øµ Ø§Ù„ØµÙØ­Ø§Øª
        for page in data.get('pages', []):
            content = page.get('content', '').strip()
            if not content or len(content) < 10:
                report['empty_pages'] += 1
            
            # ÙØ­Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            if any(ord(c) >= 0x0600 and ord(c) <= 0x06FF for c in content):
                report['arabic_content'] += 1
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
        if report['total_pages'] > 0:
            report['avg_words_per_page'] = report['total_words'] // report['total_pages']
            report['quality_score'] = ((report['total_pages'] - report['empty_pages']) / report['total_pages'] * 100)
        
        return report
        
    except Exception as e:
        return {
            'file_readable': False,
            'error': str(e),
            'quality_score': 0
        }

def print_quality_report(report):
    """Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©"""
    if not report['file_readable']:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {report.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
        return
    
    print(f"ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©:")
    print(f"  ğŸ“„ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙØ­Ø§Øª: {report['total_pages']}")
    print(f"  ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {report['total_words']:,}")
    print(f"  ğŸ“– ØµÙØ­Ø§Øª Ø¨Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ: {report['arabic_content']}")
    print(f"  âš ï¸ ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ©: {report['empty_pages']}")
    print(f"  ğŸ“Š Ù…ØªÙˆØ³Ø· ÙƒÙ„Ù…Ø§Øª/ØµÙØ­Ø©: {report['avg_words_per_page']}")
    print(f"  ğŸ¯ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©: {report['quality_score']:.1f}%")
    
    if report['quality_score'] >= 95:
        print("  âœ… Ø¬ÙˆØ¯Ø© Ù…Ù…ØªØ§Ø²Ø©!")
    elif report['quality_score'] >= 85:
        print("  âœ… Ø¬ÙˆØ¯Ø© Ø¬ÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹")
    elif report['quality_score'] >= 75:
        print("  âš ï¸ Ø¬ÙˆØ¯Ø© Ù…Ù‚Ø¨ÙˆÙ„Ø©")
    else:
        print("  âŒ Ø¬ÙˆØ¯Ø© Ù…Ù†Ø®ÙØ¶Ø© - ÙŠÙÙ†ØµØ­ Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(
        description="ğŸ›¡ï¸ Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  %(prog)s extract 12106 --max-pages 50
  %(prog)s extract 43 --output-dir my_books
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©')
    
    # Ø£Ù…Ø± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    extract_parser = subparsers.add_parser('extract', help='Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯')
    extract_parser.add_argument('book_id', help='Ù…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©')
    extract_parser.add_argument('--max-pages', type=int, help='Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙØ­Ø§Øª')
    extract_parser.add_argument('--output-dir', help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    if args.command == 'extract':
        return extract_book_command(args)
    else:
        print(f"âŒ Ø£Ù…Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {args.command}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
