#!/usr/bin/env python3
"""
Ø³ÙƒØ±Ø¨Øª ØªØ´ØºÙŠÙ„ Ù…Ø­Ø³Ù† ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© - Ultra Reliable Enhanced Runner
Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100% Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
"""

import sys
import os
import argparse
import logging
from datetime import datetime
from pathlib import Path
import time
import json
from typing import Optional

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø³Ø§Ø±
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from ultra_reliable_extractor import (
        extract_book_ultra_reliable, 
        UltraReliableConfig, 
        UltraReliableExtractor
    )
    from ultra_reliability_system import create_ultra_reliable_config
    from enhanced_database_manager import save_enhanced_json_to_database
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
except ImportError as e:
    print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø§Øª: {e}")
    print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª ultra_reliable_extractor.py Ùˆ ultra_reliability_system.py")
    sys.exit(1)

# ØªØ¹Ø±ÙŠÙ DatabaseConfig
class DatabaseConfig:
    """ØªÙƒÙˆÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    def __init__(self, host='localhost', port=3306, user='root', password='', 
                 database='shamela', charset='utf8mb4', autocommit=True,
                 connect_timeout=30, read_timeout=60, write_timeout=60,
                 pool_size=20, pool_reset_session=True, pool_pre_ping=True):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database
        self.charset = charset
        self.autocommit = autocommit
        self.connect_timeout = connect_timeout
        self.read_timeout = read_timeout
        self.write_timeout = write_timeout
        self.pool_size = pool_size
        self.pool_reset_session = pool_reset_session
        self.pool_pre_ping = pool_pre_ping

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
def setup_advanced_logging(debug_mode: bool = False):
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…"""
    
    log_level = logging.DEBUG if debug_mode else logging.INFO
    
    # ØªÙƒÙˆÙŠÙ† Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ³Ø¬ÙŠÙ„
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - [%(name)s:%(threadName)s] - %(message)s',
        handlers=[
            logging.FileHandler('ultra_reliable_runner.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # ØªØ³Ø¬ÙŠÙ„ Ù…Ù†ÙØµÙ„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡
    error_handler = logging.FileHandler('errors.log', encoding='utf-8')
    error_handler.setLevel(logging.ERROR)
    error_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(name)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    error_handler.setFormatter(error_formatter)
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³Ø¬Ù„Ø§Øª
    for logger_name in ['ultra_reliable_extractor', 'ultra_reliability_system', '__main__']:
        logger = logging.getLogger(logger_name)
        logger.addHandler(error_handler)

logger = logging.getLogger(__name__)

def print_ultra_header():
    """Ø·Ø¨Ø§Ø¹Ø© Ø±Ø£Ø³ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
    print("=" * 80)
    print("ğŸš€ Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©")
    print("Ultra Reliable Shamela Scraper v3.0")
    print("Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100% â€¢ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù… â€¢ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª")
    print("=" * 80)
    print()

def print_separator():
    """Ø·Ø¨Ø§Ø¹Ø© ÙØ§ØµÙ„"""
    print("-" * 80)

def create_optimal_config(book_size_hint: Optional[int] = None) -> UltraReliableConfig:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ù…Ø«Ù„ Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„ÙƒØªØ§Ø¨"""
    
    reliability_config = create_ultra_reliable_config()
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
    if book_size_hint:
        if book_size_hint < 50:
            # ÙƒØªØ¨ ØµØºÙŠØ±Ø© - Ø³Ø±Ø¹Ø© Ù‚ØµÙˆÙ‰
            config = UltraReliableConfig(
                reliability=reliability_config,
                max_workers=20,
                batch_size=10,
                request_delay=0.05,
                adaptive_delay=True,
                enable_smart_caching=True,
                cache_duration=7200,  # Ø³Ø§Ø¹ØªÙŠÙ†
                verify_data_integrity=True,
                validate_html_structure=True,
                check_content_quality=True,
                enable_progressive_loading=True,
                checkpoint_interval=20,
                quality_threshold=0.98
            )
        elif book_size_hint < 500:
            # ÙƒØªØ¨ Ù…ØªÙˆØ³Ø·Ø© - ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
            config = UltraReliableConfig(
                reliability=reliability_config,
                max_workers=16,
                batch_size=15,
                request_delay=0.08,
                adaptive_delay=True,
                enable_smart_caching=True,
                cache_duration=3600,  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
                verify_data_integrity=True,
                validate_html_structure=True,
                check_content_quality=True,
                enable_progressive_loading=True,
                checkpoint_interval=25,
                quality_threshold=0.95
            )
        else:
            # ÙƒØªØ¨ ÙƒØ¨ÙŠØ±Ø© - Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ù‚ØµÙˆÙ‰
            config = UltraReliableConfig(
                reliability=reliability_config,
                max_workers=12,
                batch_size=20,
                request_delay=0.1,
                adaptive_delay=True,
                enable_smart_caching=True,
                cache_duration=1800,  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
                verify_data_integrity=True,
                validate_html_structure=True,
                check_content_quality=True,
                enable_progressive_loading=True,
                checkpoint_interval=30,
                quality_threshold=0.99,
                max_empty_pages=3
            )
    else:
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…ØªÙˆØ§Ø²Ù†
        config = UltraReliableConfig(
            reliability=reliability_config,
            max_workers=16,
            batch_size=15,
            request_delay=0.08,
            adaptive_delay=True,
            enable_smart_caching=True,
            verify_data_integrity=True,
            enable_progressive_loading=True
        )
    
    return config

def extract_book_ultra_reliable_cli(book_id: str, max_pages: Optional[int] = None, 
                                   output_dir: Optional[str] = None, 
                                   config: Optional[UltraReliableConfig] = None) -> dict:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100% Ù…Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±
    """
    if config is None:
        config = create_optimal_config(max_pages)
    
    print(f"ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {book_id}")
    print(f"âš¡ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©:")
    print(f"   ğŸ”§ Ø¹Ù…Ø§Ù„: {config.max_workers}")
    print(f"   ğŸ“¦ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©: {config.batch_size}")
    print(f"   â±ï¸ ØªØ£Ø®ÙŠØ±: {config.request_delay}s")
    print(f"   ğŸ›¡ï¸ Ù…Ø­Ø§ÙˆÙ„Ø§Øª: {config.reliability.max_retries}")
    print(f"   ğŸ’¾ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª: {'Ù…ÙÙØ¹Ù„' if config.enable_smart_caching else 'Ù…Ø¹Ø·Ù„'}")
    print(f"   âœ… ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {'Ù…ÙÙØ¹Ù„' if config.verify_data_integrity else 'Ù…Ø¹Ø·Ù„'}")
    print(f"   ğŸ“Š Ø¹ØªØ¨Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©: {config.quality_threshold:.1%}")
    print_separator()
    
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨
        print("ğŸ“– Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©...")
        start_time = time.time()
        
        book_data = extract_book_ultra_reliable(book_id, max_pages, config)
        
        extraction_time = time.time() - start_time
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        pages_count = len(book_data.get('pages', []))
        words_count = sum(page.get('word_count', 0) for page in book_data.get('pages', []))
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        if not output_dir:
            output_dir = os.path.join(current_dir, "ultra_reliable_books")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ultra_reliable_book_{book_id}_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø¶ØºØ·
        print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        try:
            import gzip
            with gzip.open(filepath + '.gz', 'wt', encoding='utf-8') as f:
                json.dump(book_data, f, ensure_ascii=False, indent=2)
            filepath = filepath + '.gz'
        except ImportError:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(book_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨ ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙÙŠ {filepath}")
        
        print("\nâœ… ØªÙ… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%!")
        print_separator()
        
        # Ø·Ø¨Ø§Ø¹Ø© ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨
        print(f"ğŸ“š Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {book_data.get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        
        authors = book_data.get('authors', [])
        if authors:
            authors_names = [author.get('name', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯') for author in authors]
            print(f"ğŸ‘¨â€ğŸ“ Ø§Ù„Ù…Ø¤Ù„Ù(ÙˆÙ†): {', '.join(authors_names)}")
        
        publisher = book_data.get('publisher')
        if publisher and publisher.get('name'):
            print(f"ğŸ¢ Ø§Ù„Ù†Ø§Ø´Ø±: {publisher['name']}")
            if publisher.get('location'):
                print(f"ğŸ“ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {publisher['location']}")
        
        section = book_data.get('book_section')
        if section and section.get('name'):
            print(f"ğŸ“‚ Ø§Ù„Ù‚Ø³Ù…: {section['name']}")
        
        print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {pages_count}")
        print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {words_count:,}")
        print(f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {extraction_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸš€ Ø§Ù„Ø³Ø±Ø¹Ø©: {pages_count/extraction_time:.2f} ØµÙØ­Ø©/Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ’¾ ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ: {filepath}")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
        extractor = UltraReliableExtractor(config)
        stats = extractor.get_stats()
        if stats['pages_processed'] > 0:
            print(f"âœ… Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_rate']:.2f}%")
            print(f"ğŸ’¾ Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª: {stats['cache_hit_rate']:.2f}%")
        
        print_separator()
        
        return book_data
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {str(e)}", exc_info=True)
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {str(e)}")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
        print("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©...")
        try:
            from ultra_reliability_system import BackupManager
            backup_manager = BackupManager(config.reliability)
            backup_data = backup_manager.restore_from_backup(book_id)
            if backup_data:
                print("âœ… ØªÙ… Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©!")
                return backup_data
        except Exception:
            pass
        
        raise

def save_to_database_ultra_reliable(json_path: str, db_config: DatabaseConfig = None) -> bool:
    """Ø­ÙØ¸ Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø¹Ø§Ù„ÙŠØ©"""
    
    print("ğŸ—„ï¸ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    print(f"ğŸ“ Ø§Ù„Ù…Ø³Ø§Ø±: {json_path}")
    print_separator()
    
    try:
        # ØªÙƒÙˆÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
        if db_config is None:
            db_config = DatabaseConfig()
        
        # ØªÙƒÙˆÙŠÙ† Ù…ÙˆØ«ÙˆÙ‚ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        reliable_db_config = DatabaseConfig(
            host=db_config.host,
            port=db_config.port,
            user=db_config.user,
            password=db_config.password,
            database=db_config.database,
            charset='utf8mb4',
            autocommit=True,
            connect_timeout=30,
            read_timeout=60,
            write_timeout=60,
            pool_size=20,
            pool_reset_session=True,
            pool_pre_ping=True
        )
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                result = save_enhanced_json_to_database(
                    json_path=json_path,
                    db_config=reliable_db_config,
                    performance_config=None
                )
                
                if result:
                    print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
                    return True
                    
            except Exception as e:
                logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {str(e)}")
                if attempt < max_attempts - 1:
                    wait_time = 5 * (attempt + 1)
                    print(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {wait_time} Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
                    return False
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
        print(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
        return False

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    parser = argparse.ArgumentParser(
        description="Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© ÙØ§Ø¦Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© - Ultra Reliable Shamela Scraper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  %(prog)s extract 12106 --max-pages 50
  %(prog)s extract 43 --max-pages 100 --output-dir my_books
  %(prog)s save-db book.json.gz --db-password mypass
  %(prog)s extract 12106 --debug --quality-threshold 0.99
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©')
    
    # Ø£Ù…Ø± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    extract_parser = subparsers.add_parser('extract', help='Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªØ§Ø¨ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%')
    extract_parser.add_argument('book_id', help='Ù…Ø¹Ø±Ù Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©')
    extract_parser.add_argument('--max-pages', type=int, help='Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙØ­Ø§Øª')
    extract_parser.add_argument('--output-dir', help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬')
    extract_parser.add_argument('--workers', type=int, default=16, help='Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ø§Ù„')
    extract_parser.add_argument('--batch-size', type=int, default=15, help='Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©')
    extract_parser.add_argument('--delay', type=float, default=0.08, help='Ø§Ù„ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª')
    extract_parser.add_argument('--quality-threshold', type=float, default=0.95, help='Ø¹ØªØ¨Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©')
    extract_parser.add_argument('--no-cache', action='store_true', help='ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª')
    extract_parser.add_argument('--no-validation', action='store_true', help='ØªØ¹Ø·ÙŠÙ„ ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    extract_parser.add_argument('--debug', action='store_true', help='ÙˆØ¶Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­')
    
    # Ø£Ù…Ø± Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    db_parser = subparsers.add_parser('save-db', help='Ø­ÙØ¸ ÙƒØªØ§Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    db_parser.add_argument('json_path', help='Ù…Ø³Ø§Ø± Ù…Ù„Ù JSON')
    db_parser.add_argument('--db-host', default='localhost', help='Ø¹Ù†ÙˆØ§Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    db_parser.add_argument('--db-port', type=int, default=3306, help='Ù…Ù†ÙØ° Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    db_parser.add_argument('--db-user', default='root', help='Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…')
    db_parser.add_argument('--db-password', help='ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    db_parser.add_argument('--db-name', default='shamela', help='Ø§Ø³Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª')
    
    # Ø£Ù…Ø± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats_parser = subparsers.add_parser('stats', help='Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…')
    stats_parser.add_argument('--detailed', action='store_true', help='Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©')
    
    args = parser.parse_args()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    setup_advanced_logging(getattr(args, 'debug', False))
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø±Ø£Ø³
    print_ultra_header()
    
    if args.command == 'extract':
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø®ØµØµ
            config = create_optimal_config(args.max_pages)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©
            if hasattr(args, 'workers'):
                config.max_workers = args.workers
            if hasattr(args, 'batch_size'):
                config.batch_size = args.batch_size
            if hasattr(args, 'delay'):
                config.request_delay = args.delay
            if hasattr(args, 'quality_threshold'):
                config.quality_threshold = args.quality_threshold
            if hasattr(args, 'no_cache') and args.no_cache:
                config.enable_smart_caching = False
            if hasattr(args, 'no_validation') and args.no_validation:
                config.verify_data_integrity = False
                config.validate_html_structure = False
                config.check_content_quality = False
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨
            book = extract_book_ultra_reliable_cli(
                book_id=args.book_id,
                max_pages=args.max_pages,
                output_dir=args.output_dir,
                config=config
            )
            
            print("ğŸ‰ ØªÙ…Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%!")
            
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}", exc_info=True)
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
            sys.exit(1)
    
    elif args.command == 'save-db':
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if not args.db_password:
                import getpass
                args.db_password = getpass.getpass("ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: ")
            
            db_config = DatabaseConfig(
                host=args.db_host,
                port=args.db_port,
                user=args.db_user,
                password=args.db_password,
                database=args.db_name
            )
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            success = save_to_database_ultra_reliable(args.json_path, db_config)
            
            if success:
                print("ğŸ‰ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
            else:
                print("âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                sys.exit(1)
                
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}", exc_info=True)
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            sys.exit(1)
    
    elif args.command == 'stats':
        print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:")
        print("- Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© 100%")
        print("- Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…ÙÙØ¹Ù„Ø©")
        print("- Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù…ØªØ§Ø­Ø©")
        print("- Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø°ÙƒÙŠ Ù…ØªØ§Ø­")
        print("- ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…ÙÙØ¹Ù„")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
